# Monitoring and Logging for Event Handling\n\n## Overview\n\nEffective monitoring and comprehensive logging are vital for maintaining the health, performance, and reliability of the Search Service\'s event handling capabilities. They enable operators to understand system behavior, diagnose issues, track down problematic events, and ensure that the search indexes are being updated correctly and efficiently.\n\n## Key Areas for Monitoring\n\n1.  **Event Consumption Rate**: Number of events consumed per unit of time from each Kafka topic (or other message sources).\n2.  **Event Processing Rate**: Number of events successfully processed and indexed per unit of time.\n3.  **Error Rates**:\n    *   Number and type of errors during deserialization/validation.\n    *   Number and type of errors during event transformation.\n    *   Number and type of errors during Elasticsearch operations (indexing, updates, deletes).\n    *   Rate of messages sent to Dead Letter Queues (DLQs).\n4.  **Consumer Lag (Kafka)**:\n    *   The difference in offsets between the last message produced to a topic partition and the last message consumed by the Search Service for that partition.\n    *   High or consistently growing lag indicates the service is not keeping up with event production.\n    *   Tools: Kafka-specific monitoring tools (e.g., Confluent Control Center, Prometheus JMX exporter for Kafka, custom Kafka consumer lag scripts).\n5.  **Processing Latency**: Time taken to process an event from consumption to successful indexing.\n    *   Track average, median, and percentile (e.g., p95, p99) latencies.\n6.  **DLQ Size and Age**: Number of messages in the DLQ and the age of the oldest message. A growing DLQ or very old messages require immediate attention.\n7.  **Elasticsearch Performance**: Monitor key metrics from the Elasticsearch cluster itself that could be impacted by or impact indexing:\n    *   Indexing rate (documents/second).\n    *   Indexing latency.\n    *   Bulk indexing request success/failure rates and latencies.\n    *   CPU/Memory/Disk I/O on Elasticsearch nodes.\n    *   Cluster health (green, yellow, red).\n8.  **Resource Utilization of Search Service Instances**: CPU, memory, network I/O of the service instances handling event consumption.\n\n## Logging Strategy\n\nStructured logging (e.g., JSON format) is highly recommended as it makes logs easier to parse, search, and analyze by log management systems.\n\n### Key Information to Log for Each Event Processed:\n\n*   **Timestamp**: Precise time of the log entry.\n*   **Log Level**: INFO, WARN, ERROR, DEBUG.\n*   **Event ID**: Unique identifier of the event being processed (from the event payload).\n*   **Event Type**: e.g., `ProductCreated`, `CategoryUpdated`.\n*   **Source Topic/Partition/Offset (Kafka)**: For traceability back to the message broker.\n*   **Processing Stage**: e.g., `Consumed`, `Validated`, `Transformed`, `IndexingAttempt`, `IndexedSuccess`, `IndexingFailed`, `SentToDLQ`.\n*   **Duration**: Time taken for critical stages (e.g., transformation duration, ES indexing duration).\n*   **Outcome**: Success or Failure.\n*   **Error Details (if applicable)**:\n    *   Error message.\n    *   Error type/code.\n    *   Stack trace (especially for unexpected errors).\n    *   Relevant context from the event (e.g., `productId`, `categoryId` - be careful not to log overly verbose or sensitive PII data from the event payload directly in all logs unless necessary for debugging and properly secured).\n*   **Correlation ID**: A unique ID that can trace a single event\'s journey through multiple processing steps or even across microservices if distributed tracing is implemented.\n\n### Log Levels Guidance:\n\n*   **INFO**: Routine processing steps, successful indexing of an event, summary of batch operations.\n    *   Example: `Event ${eventId} (${eventType}) consumed from ${topic}.`\n    *   Example: `Product ${productId} indexed successfully from event ${eventId}.`\n*   **WARN**: Potentially problematic situations that don\'t stop processing but should be noted. Minor validation issues that have fallbacks, successful retries after initial failures, 404s on delete operations (already deleted).\n    *   Example: `Event ${eventId} - field \'optionalField\' missing, using default.`\n    *   Example: `Elasticsearch delete for product ${productId} (event ${eventId}) returned 404, likely already deleted.`\n*   **ERROR**: Failures in processing an event, sending to DLQ, unexpected exceptions.\n    *   Example: `Failed to index product ${productId} from event ${eventId} after ${retries} retries. Sending to DLQ. Error: ${errorMessage}`\n    *   Example: `Deserialization failed for message from ${topic} at offset ${offset}. Payload: ${rawPayloadSnippet}` (Careful with payload logging).\n*   **DEBUG**: Detailed information for troubleshooting, such as full transformed documents before indexing, detailed steps within a complex transformation. Usually disabled in production but can be enabled dynamically if needed.\n\n### NestJS Logging Implementation:\n\nNestJS uses a built-in `Logger` service. This can be extended or replaced with more sophisticated logging libraries like `Pino` or `Winston` for structured JSON logging and better performance.\n\n**Example using built-in Logger (can be customized):**\n```typescript\nimport { Injectable, Logger, Scope } from \'@nestjs/common\';\n\n@Injectable({ scope: Scope.TRANSIENT }) // Or default singleton scope\nexport class MyService {\n  private readonly logger = new Logger(MyService.name);\n  // For more context, you can pass a custom context to the Logger constructor\n  // private readonly logger = new Logger(\`MyService[${contextId}]\`);\n\n  processEvent(event: any) {\n    const eventId = event.eventId || \'unknown-event-id\';\n    this.logger.log(`Processing event ${eventId}...`, \'EventProcessing\'); // Optional custom context for log entry\n\n    try {\n      // ... processing logic ...\n      if (success) {\n        this.logger.log(`Event ${eventId} processed successfully.`, \'EventProcessingSuccess\');\n      } else {\n        throw new Error(\'Simulated processing error\');\n      }\n    } catch (error) {\n      this.logger.error(\n        `Failed to process event ${eventId}: ${error.message}`,\n        error.stack,\n        \'EventProcessingFailure\',\n      );\n      // Potentially re-throw or handle by sending to DLQ\n    }\n  }\n}\n```\n\n## Monitoring and Logging Tools\n\n*   **Log Management**: ELK Stack (Elasticsearch, Logstash, Kibana), Splunk, Grafana Loki, Datadog Logs, AWS CloudWatch Logs.\n*   **Metrics & Monitoring**: Prometheus & Grafana, Datadog, Dynatrace, New Relic, AWS CloudWatch Metrics.\n*   **Distributed Tracing (Advanced)**: Jaeger, Zipkin. Useful if an event triggers a cascade of actions across multiple services and you need to trace its entire lifecycle.\n*   **Alerting**: Integrated with monitoring tools (e.g., Prometheus Alertmanager, Datadog Monitors, CloudWatch Alarms).\n\n## Key Performance Indicators (KPIs) for Event Handling\n\n*   **Event Throughput**: Events processed per second/minute.\n*   **End-to-End Event Latency**: Time from event publication (source service) to visibility in search (requires coordination for measurement).\n*   **Error Rate Percentage**: (Failed Events / Total Events) * 100%.\n*   **DLQ Growth Rate**: Messages added to DLQ per hour/day.\n*   **Consumer Lag**: Max and average lag across all partitions.\n*   **System Resource Utilization**: CPU/Memory of consumer instances.\n\n## Next Steps\n\n*   `10-data-consistency-strategies.md`: Addresses how to manage and ensure data consistency between source systems and the search index in an eventually consistent model.\n